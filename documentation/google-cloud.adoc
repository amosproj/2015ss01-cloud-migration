Google Cloud
============


== App Engine ==

As discussed, the application was intended to be written for the Google App Engine [GAE]. But soon we realized, that GAE is not inteded or adaptable for our implementation of the multi-cloud.

For example, GAE has many restrictions:

* GAE does not support native-compiled libraries, like python-etcd, etc.
* GAE only supports externl connections for about max. 60 seconds
* GAE can't use multiproccessing

These features, especially the last 2 are an essentiall part in our Multi-cloud-implementation, because we need to listen for ACKS of the other two clouds. Our current implementation uses Multiproccessing for listening an indefinite time on another server.

So we were forced to try another approach. This was also a good oppurtunity to show for this project, how fast you can integrate another cloud-system into the project.  

== Compute Engine ==

== Basic Approach ==

The more basic approach was to use Google Compute Engine. There you can choose from a variaty of images and hardware architecture a new system.

In our case we used a micro instance, with Debian 6 [wheezy]. Inital setup is very hard, because our application i just a server backend, not the web-sever.

For a web-server-proxy we used nginx and for actual parallel processing of the requests we used gunicorn.

After these system are setup. [Just normally set them up, via commandline] We need to use virtualenv to hold changes locally in the project and afterwards install every dependency via pip. At last we are cloning the project into the virtualenv and install the remaining dependencies.

Afterwards we need to load the configs from our secret repo, where secret keys are stored, etc and need to configure the configs for nginx/gunicorn/etc.

== Actual Approach ==

Usally you don't do these steps manually. Instead you need, especially for load-balancing and autoscaling, templates which can start your instance fast and without complications.

First you need to setup a instance-template:
* At first you decide which machine you gonna use. Here you have to pay attention to the computational power and the RAM. This will have affect on the later loadbalancing in the Network
* Afterwards you have to ensure a connection to the Storage of the cloud and the Database in MYSQL
* Third you need to include a startupscript, which ensures the right librarys are installed and configured right

During Setup you need to assign to Key/pair values:
* startup-script-url, [gs://bucket/script]
 
Which should have the value of the script you want to use for the init-procedure.
We used:

* startup-script-url gs://instance-loadingscripts/startupscript.sh

Exmaple Autoscript: [we are using this file]

 ---- 
set -e

copy_arr(){
    array=$1[@]
    arr=("${!array}")
    for file in ${arr[@]};
    do
        curl -L -O -J "https://drive.google.com/uc?export=download&id=${file}"
    done
}


#config.py
google[0]="XXX"

#brutto-netto-rechner-compute.pem?
google[1]="XXX"

#server.conf
google[2]="XXX"

#nginx.conf
google[3]="XXX"

cd /home
mkdir sys
cd sys

wget "https://bootstrap.pypa.io/get-pip.py"
python get-pip.py

sudo apt-get -y update
sudo apt-get -y install nginx
sudo apt-get -y install git
sudo apt-get -y install libmysqlclient-dev
sudo apt-get -y install python-dev

pip install virtualenv
mkdir environments
sudo virtualenv /home/sys/environments/amos
source /home/sys/environments/amos/bin/activate
cd /home/sys/environments/amos
pip install gunicorn

#get project
sudo git clone "https://github.com/ohaz/amos-ss15-proj1.git"
cd amos-ss15-proj1

#app.yaml
pip install gcloud
pip install jinja2
pip install pycrypto
pip install mysql

#install requirements
pip install -r requirements.txt

#google storage librarys
pip install google-api-python-client

#get secrets
copy_arr google
cp server.conf ../server.conf
sudo cp nginx.conf /etc/nginx/nginx.conf
sudo /usr/sbin/nginx

../bin/gunicorn -w 4 FlaskWebProject:app &

 ---- 

After a template is setup, you need to ensure autoscaling and loadbalancing. For these tasks google explains everything in detail on their website. Bascially you just have to set up a group, with your previous constructed template. On Creation you have to pay attention to enable autoscaling. You configure here, when and why another instance shall be started. Here we are using usage of CPU-computation as an indicator, and if more the 80% in the average are used, we are starting a new instance.

For load-balancing you need to setup a load-balancer.This loadbalancer gives you an adress which shall be used for accessing you application. In this loadbalancer you have to setup the method of load-balancing. We are also balancing via Request per Seconds. You also have to setup the backend of the loadbalancer, which is the group, we previously constructed.


== DB-Connection and Storage-adapter ==

These points need configuring to. First of all we need to setup up the Database. Here we take the approach via alembic, which can use the internal Data of our Python-application to generate a Database. But here we need to be carful. If our application[instance] has only an IP4-address, we need to ensure an IP4 for the DB in the cloud! 
After the inital Database is setup in the cloud[see google documentation], we have to adapt the config with the according address. At last we generate via alembic the Database.[see alembic docu] [see config.py section]
For the storage-setup we need an accoring implementation to our interface. [see other sections]

== Cloud Deployer ==

The Cloud Deployer is a very fast way to deploy your code to the clouds, but because security is a very high standard at google, we still need to setup a little but more.

=== Setting up, to use google for the deployer ===

If you want to use the deployer for the google Compute Engine, follow these steps and you should be good to go:

Google Compute engine is server based. So for the use of the deployer make sure, you have a server already running in the cloud. You should have also followed the steps discribed in the other Tutorial about setting up a template in the Google Compute Engine and use this for a fast start of the Http-Server.

We will deploy our application via ssh, and google only allows connection with an RSA-Key. Therfore you need to apply:

 ---- 
ssh-keygen -f google_rsa -t rsa -N ''
 ----

Instead of google_rsa, you can choose any name you want, but make sure in the follwing steps to replace the right parts of the expressions.
This will generate a private- and a public-key for encryption. You have to upload you public-key to the instance. You can either to this via another scp connection [which needs to be already setup], if you can't do this, go into the developer-sconsole and add the key there in your instance. [see google tutorials] Another approach via the Google-API/Libaray/gcloud can also be taken.


Make sure your generated files are also placed in ~/.ssh/ ,because this is important for the next step.

ssh uses configs for a fast connection-setup. So does git. Therefore we need this to be setup.
Create a file config in ~/.ssh/ [~/.ssh/config] with following content:

 ---- 

Host google_compute
    HostName [IP]
    User [user]
    IdentityFile ~/.ssh/google_rsa
    UserKnownHostsFile /dev/null
    CheckHostIP no
    StrictHostKeyChecking 

 ---- 

For the [IP] insert the public IP of the instance [or load-balancer], which you can see in the developers-console. For [user], you insert the same part which was generated in your public-key:

 ---- 
ssh-rsa XXX-INCREDIBLY_LONG_I_MEAN_REALLY_LONG_KEY_XXX User@SomeDevice
 ---- 

Now you should be able to ssh to the server, via 
 ---- 
ssh google_compute
 ---- 
It is very important that you name the host google_compute, if you can't use this name. Make sure you replace the accoring code, in the repository [Library] (Should be one line)

After this is done, you are finished. You should now be able to deploy in the path cloud-deployer via:
 ----
python deployer.py
 ----

Have fun with the AMOS-Deployer!

